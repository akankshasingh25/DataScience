# -*- coding: utf-8 -*-
"""Akanksha_Singh_Assignment1_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rc-mRN6VmTjSbPesCA0Mor6w84a4Sx_E
"""

# importing library
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd
import csv
import sys

# reading training and test dataset
df1 = pd.read_csv('training_data.csv', error_bad_lines = False, header= None)
df2 = pd.read_csv('test_data.csv', error_bad_lines = False, header= None)
df3 = pd.read_csv('training_data_class_labels.csv', error_bad_lines = False, header= None)

"""*1. Plot the given training data using different colors for individual classes on the console in order to visualize it.*"""

# concatenating dataframes
df1.rename(columns={0:"A", 1:"B"}, inplace= True)
df3.rename(columns ={0: 'Label'}, inplace= True)
df = pd.concat([df1, df3], axis=1)
df

# plotting the training data
colors = ["red", "yellow"]
colormap = mpl.colors.ListedColormap(colors)
plt.scatter(df.A, df.B, c= df.Label, cmap=colormap, s=8, alpha = 0.5)

"""*2.* *Perform naive bayes, logistic regression and k-nearest neighbor classifiers to categorize the instances of the given test data. Initially, find the best performance for each of these classifiers by properly tuning its parameters as discussed in the class.*"""

# importing sklearn library
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn import linear_model
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RepeatedStratifiedKFold

"""## Training and prediction without tuning"""

data = df[["A","B"]]
lbl = df.Label

# splitting training data
trn_data, test_data, trn_lbl, test_lbl = train_test_split(data, lbl, test_size = 0.3, random_state = 66, stratify = lbl)

# naive bayes classifier

nb = GaussianNB()
nb.fit(trn_data, trn_lbl)
nb_pred_wo_tune = nb.predict(test_data)
print(classification_report(test_lbl, nb_pred_wo_tune))

# logistic regression classifier

log = linear_model.LogisticRegression()
log.fit(trn_data, trn_lbl)
log_pred_wo_tune = log.predict(test_data)
print(classification_report(test_lbl, log_pred_wo_tune))

#knn classifier

knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(trn_data, trn_lbl)
knn_pred_wo_tune = knn.predict(test_data)
print(classification_report(test_lbl, knn_pred_wo_tune))

"""## Training and prediction with tuning"""

model_params = {
    
    'naive_bayes_gaussian': {
        'model': GaussianNB(),
        'params': {
            'var_smoothing' : [1e-15, 1e-13, 1e-11, 1e-9, 1e-7, 1e-5, 1e-3]
        }
    },

    'logistic_regression' : {
        'model': LogisticRegression(),
        'params': {
            'C': [1e-2, 1e-1, 1 ,5 ,10],
            'solver': ['liblinear','netwon-cg', 'lbfgs']
        }
    },

    'knn_classifier': {
        'model': KNeighborsClassifier(),
        'params': {
            'n_neighbors': [1, 3, 5, 7, 9],
            'weights': ['uniform', 'distance'],
            'metric': ['minkowski', 'manhattan']
        }
    }     
}

scores = []

for model_name, mp in model_params.items():
    clf =  GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)
    clf.fit(data, lbl)
    scores.append({
        'model': model_name,
        'best_score': clf.best_score_,
        'best_params': clf.best_params_
    })
    
df = pd.DataFrame(scores,columns=['model','best_score','best_params'])
df

# with tuning best predictions

knn = KNeighborsClassifier(metric = 'minkowski', n_neighbors = 1, weights = 'uniform')
knn.fit(trn_data, trn_lbl)
knn_pred_w_tune = knn.predict(test_data)
print(classification_report(test_lbl, knn_pred_w_tune))

"""*3. The performance of your model will be evaluated in terms the macro-averaged f-measure following this library.*

# Best Classifier: KNN
Using this on given test data
"""

final_model = KNeighborsClassifier(metric = 'minkowski', n_neighbors = 1, weights = 'uniform' )
final_model.fit(data,lbl)
lbl_final_model_pred = knn.predict(df2)
lbl_final_model_pred

np.savetxt("Akanksha_Singh_Assignment1_testdata_classlabels.csv", lbl_final_model_pred, fmt="%i")